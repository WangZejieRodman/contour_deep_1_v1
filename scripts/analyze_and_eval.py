"""
Day 9: åŸºäºéªŒè¯é›†çš„è·¨æ—¶é—´æ®µè¯„ä¼°ï¼ˆä½¿ç”¨BEVç¼“å­˜ï¼‰
å‚è€ƒ analyze_and_eval_day9.py çš„ç»“æ„
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from tqdm import tqdm
import pickle
import yaml
from sklearn.neighbors import NearestNeighbors
import json
from pathlib import Path


def load_pickle(filepath):
    """åŠ è½½pickleæ–‡ä»¶"""
    with open(filepath, 'rb') as f:
        data = pickle.load(f)
    print(f"âœ“ Loaded: {filepath}")
    return data


def extract_features_from_cache(model, data_sets, cache_root, cache_type, device):
    """
    ä»BEVç¼“å­˜æå–ç‰¹å¾

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_sets: List[Dict], æ¯ä¸ªsessionçš„æ•°æ®å­—å…¸
        cache_root: BEVç¼“å­˜æ ¹ç›®å½•
        cache_type: 'evaluation_database' æˆ– 'evaluation_query'
        device: è®¾å¤‡

    Returns:
        features: [N, 128] æ‰€æœ‰ç‰¹å¾
        metadata: List[Dict] æ¯ä¸ªç‰¹å¾çš„å…ƒæ•°æ®
    """
    from data.data_utils import normalize_bev, stack_bev_with_vcd

    print(f"\n[æå–{cache_type}ç‰¹å¾]")
    print("=" * 60)

    features = []
    metadata = []
    global_idx = 0

    model.eval()
    with torch.no_grad():
        for session_idx, data_dict in enumerate(tqdm(data_sets, desc=f"{cache_type} Sessions")):
            if len(data_dict) == 0:
                continue

            session_cache_dir = os.path.join(cache_root, cache_type, f'session_{session_idx}')

            if not os.path.exists(session_cache_dir):
                print(f"  âš ï¸  Session {session_idx} ç¼“å­˜ä¸å­˜åœ¨: {session_cache_dir}")
                continue

            for local_idx in tqdm(sorted(data_dict.keys()),
                                 desc=f"  Session {session_idx}",
                                 leave=False):
                entry = data_dict[local_idx]

                # ç¼“å­˜æ–‡ä»¶è·¯å¾„
                cache_filename = f"{local_idx:06d}.npz"
                cache_path = os.path.join(session_cache_dir, cache_filename)

                if not os.path.exists(cache_path):
                    print(f"  âš ï¸  ç¼“å­˜ä¸å­˜åœ¨: {cache_path}")
                    continue

                try:
                    # ä»ç¼“å­˜åŠ è½½BEV
                    data = np.load(cache_path)
                    bev_layers = data['bev_layers']
                    vcd = data['vcd']

                    # é¢„å¤„ç†
                    bev_norm, vcd_norm = normalize_bev(bev_layers, vcd)
                    stacked = stack_bev_with_vcd(bev_norm, vcd_norm)
                    bev_tensor = torch.from_numpy(stacked).float().unsqueeze(0).to(device)

                    # æå–ç‰¹å¾
                    feat = model(bev_tensor)
                    features.append(feat.cpu().numpy()[0])

                    # è®°å½•å…ƒæ•°æ®
                    meta = {
                        'session_idx': session_idx,
                        'local_idx': local_idx,
                        'global_idx': global_idx,
                        'file': entry['query']
                    }

                    # å¦‚æœæ˜¯æŸ¥è¯¢é›†ï¼Œæ·»åŠ æ­£æ ·æœ¬ä¿¡æ¯
                    if 'positives' in entry:
                        meta['positives'] = entry['positives']

                    metadata.append(meta)
                    global_idx += 1

                except Exception as e:
                    print(f"  âŒ å¤„ç†å¤±è´¥ {cache_path}: {e}")
                    continue

    features = np.vstack(features)
    print(f"âœ“ æå–äº† {len(features)} ä¸ªç‰¹å¾")

    return features, metadata


def map_positives_to_global(query_metadata, db_metadata):
    """
    å°†æŸ¥è¯¢çš„æ­£æ ·æœ¬ä» {session_idx: [local_indices]} æ˜ å°„åˆ°å…¨å±€ç´¢å¼•

    Args:
        query_metadata: æŸ¥è¯¢å…ƒæ•°æ®
        db_metadata: æ•°æ®åº“å…ƒæ•°æ®

    Returns:
        global_positives: List[Set[int]], æ¯ä¸ªæŸ¥è¯¢çš„å…¨å±€æ­£æ ·æœ¬é›†åˆ
    """
    print("\n[æ˜ å°„æ­£æ ·æœ¬åˆ°å…¨å±€ç´¢å¼•]")
    print("=" * 60)

    # æ„å»º (session_idx, local_idx) -> global_idx çš„æ˜ å°„
    db_local_to_global = {}
    for db_meta in db_metadata:
        key = (db_meta['session_idx'], db_meta['local_idx'])
        db_local_to_global[key] = db_meta['global_idx']

    global_positives = []
    total_positives = 0
    queries_with_positives = 0

    for query_meta in query_metadata:
        positives_set = set()

        # éå†è¯¥æŸ¥è¯¢åœ¨å„ä¸ªæ•°æ®åº“sessionä¸­çš„æ­£æ ·æœ¬
        if 'positives' in query_meta:
            for db_session_idx, local_indices in query_meta['positives'].items():
                for local_idx in local_indices:
                    key = (db_session_idx, local_idx)
                    if key in db_local_to_global:
                        global_idx = db_local_to_global[key]
                        positives_set.add(global_idx)

        global_positives.append(positives_set)
        total_positives += len(positives_set)

        if len(positives_set) > 0:
            queries_with_positives += 1

    print(f"âœ“ æ€»æŸ¥è¯¢æ•°: {len(global_positives)}")
    print(f"âœ“ æœ‰æ­£æ ·æœ¬çš„æŸ¥è¯¢æ•°: {queries_with_positives}")
    print(f"âœ“ æ— æ­£æ ·æœ¬çš„æŸ¥è¯¢æ•°: {len(global_positives) - queries_with_positives}")
    print(f"âœ“ æ€»æ­£æ ·æœ¬å¯¹æ•°: {total_positives}")

    if queries_with_positives > 0:
        print(f"âœ“ å¹³å‡æ¯æŸ¥è¯¢æ­£æ ·æœ¬æ•°: {total_positives / queries_with_positives:.1f}")

    return global_positives


def compute_recall(query_features, db_features, global_positives, k_values=[1, 5, 10, 25]):
    """
    è®¡ç®—Recall@K

    Args:
        query_features: [N_query, D]
        db_features: [N_db, D]
        global_positives: List[Set[int]], æ¯ä¸ªæŸ¥è¯¢çš„å…¨å±€æ­£æ ·æœ¬é›†åˆ
        k_values: è¦è®¡ç®—çš„Kå€¼åˆ—è¡¨

    Returns:
        recalls: Dict[int, float]
        rank_stats: Dict
    """
    print("\n[è®¡ç®—Recall@K]")
    print("=" * 60)

    # æ„å»ºKNNç´¢å¼•
    max_k = max(k_values)
    if len(db_features) < max_k:
        max_k = len(db_features)
        print(f"  âš ï¸  æ•°æ®åº“å¤§å° < {max(k_values)}ï¼Œè°ƒæ•´Kä¸º{max_k}")

    knn = NearestNeighbors(n_neighbors=max_k, metric='euclidean', n_jobs=-1)
    knn.fit(db_features)
    print(f"âœ“ KNNç´¢å¼•æ„å»ºå®Œæˆ (K={max_k})")

    recalls = {k: 0 for k in k_values}
    rank_of_first_positive = []
    valid_queries = 0

    print("æ£€ç´¢ä¸­...")
    for i in tqdm(range(len(query_features))):
        if len(global_positives[i]) == 0:
            continue  # è·³è¿‡æ²¡æœ‰æ­£æ ·æœ¬çš„æŸ¥è¯¢

        valid_queries += 1

        # KNNæœç´¢
        distances, indices = knn.kneighbors([query_features[i]])
        retrieved_indices = indices[0]

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£æ ·æœ¬çš„æ’å
        first_positive_rank = None
        for rank, idx in enumerate(retrieved_indices, start=1):
            if idx in global_positives[i]:
                first_positive_rank = rank
                break

        if first_positive_rank:
            rank_of_first_positive.append(first_positive_rank)

        # æ£€æŸ¥Recall@K
        for k in k_values:
            if k <= max_k:
                if any(idx in global_positives[i] for idx in retrieved_indices[:k]):
                    recalls[k] += 1

    # å½’ä¸€åŒ–
    if valid_queries > 0:
        for k in recalls:
            recalls[k] = recalls[k] / valid_queries * 100
    else:
        print("  âš ï¸  æ²¡æœ‰æœ‰æ•ˆæŸ¥è¯¢!")

    # ç»Ÿè®¡
    rank_stats = {}
    if rank_of_first_positive:
        rank_stats = {
            'mean': float(np.mean(rank_of_first_positive)),
            'median': float(np.median(rank_of_first_positive)),
            'min': int(np.min(rank_of_first_positive)),
            'max': int(np.max(rank_of_first_positive))
        }

    print(f"âœ“ æœ‰æ•ˆæŸ¥è¯¢æ•°: {valid_queries}")

    return recalls, rank_stats


def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("=" * 60)
    print("Day 9: åŸºäºéªŒè¯é›†çš„è·¨æ—¶é—´æ®µè¯„ä¼°")
    print("=" * 60)

    # 1. åŠ è½½æ¨¡å‹
    print("\n[1/6] åŠ è½½æ¨¡å‹...")
    from models.retrieval_net import RetrievalNet

    model = RetrievalNet(output_dim=128)
    checkpoint_path = 'checkpoints/retrieval_baseline_day8/latest.pth'

    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpointä¸å­˜åœ¨: {checkpoint_path}")
        return

    checkpoint = torch.load(checkpoint_path, map_location='cuda', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.cuda()
    model.eval()

    print(f"  âœ“ åŠ è½½Epoch {checkpoint['epoch']+1}çš„æ¨¡å‹")
    print(f"  âœ“ Val Loss: {checkpoint['metric']:.4f}")

    # 2. åŠ è½½æ•°æ®åº“å’ŒæŸ¥è¯¢é›†pickle
    print("\n[2/6] åŠ è½½æ•°æ®é›†...")
    database_pickle = '/home/wzj/pan1/contour_deep_1++/data/chilean_evaluation_database_190_194.pickle'
    query_pickle = '/home/wzj/pan1/contour_deep_1++/data/chilean_evaluation_query_195_199.pickle'

    database_sets = load_pickle(database_pickle)
    query_sets = load_pickle(query_pickle)

    print(f"  âœ“ æ•°æ®åº“sessions: {len(database_sets)}")
    print(f"  âœ“ æŸ¥è¯¢sessions: {len(query_sets)}")

    # 3. æå–æ•°æ®åº“ç‰¹å¾
    print("\n[3/6] æå–æ•°æ®åº“ç‰¹å¾...")
    cache_root = '/home/wzj/pan1/contour_deep_1++/data/Chilean_BEV_Cache'

    db_features, db_metadata = extract_features_from_cache(
        model, database_sets, cache_root, 'evaluation_database', 'cuda'
    )

    # 4. æå–æŸ¥è¯¢ç‰¹å¾
    print("\n[4/6] æå–æŸ¥è¯¢ç‰¹å¾...")
    query_features, query_metadata = extract_features_from_cache(
        model, query_sets, cache_root, 'evaluation_query', 'cuda'
    )

    # 5. æ˜ å°„æ­£æ ·æœ¬
    print("\n[5/6] æ˜ å°„æ­£æ ·æœ¬...")
    global_positives = map_positives_to_global(query_metadata, db_metadata)

    # 6. è®¡ç®—Recall
    print("\n[6/6] è®¡ç®—Recall...")
    recalls, rank_stats = compute_recall(query_features, db_features, global_positives)

    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("è¯„ä¼°ç»“æœï¼ˆè·¨æ—¶é—´æ®µï¼‰")
    print("=" * 60)
    print(f"æ•°æ®åº“: Session 190-194 ({len(db_features)} entries)")
    print(f"æŸ¥è¯¢:   Session 195-199 ({len(query_features)} entries)")
    print(f"\nRecallæ€§èƒ½:")
    print(f"  Recall@1:  {recalls[1]:.2f}%")
    print(f"  Recall@5:  {recalls[5]:.2f}%")
    print(f"  Recall@10: {recalls[10]:.2f}%")
    print(f"  Recall@25: {recalls[25]:.2f}%")

    if rank_stats:
        print(f"\nç¬¬ä¸€ä¸ªæ­£æ ·æœ¬æ’åç»Ÿè®¡:")
        print(f"  å¹³å‡æ’å: {rank_stats['mean']:.1f}")
        print(f"  ä¸­ä½æ•°æ’å: {rank_stats['median']:.1f}")
        print(f"  æœ€å°æ’å: {rank_stats['min']}")
        print(f"  æœ€å¤§æ’å: {rank_stats['max']}")

    # è¯Šæ–­ç»“è®º
    print("\n" + "=" * 60)
    print("è¯Šæ–­ç»“è®º:")
    print("=" * 60)

    if recalls[1] >= 70:
        print(f"  ğŸŒŸ è·¨æ—¶é—´æ®µæ³›åŒ–èƒ½åŠ›ä¼˜ç§€ (Recall@1 = {recalls[1]:.2f}%)")
        print(f"  âœ… æ¥è¿‘æˆ–è¶…è¿‡ä¼ ç»Ÿæ–¹æ³•")
    elif recalls[1] >= 60:
        print(f"  âœ… è·¨æ—¶é—´æ®µæ³›åŒ–èƒ½åŠ›è‰¯å¥½ (Recall@1 = {recalls[1]:.2f}%)")
        print(f"  â†’ å¯ä»¥è¿›å…¥æ–¹å‘2çš„å¼€å‘")
    elif recalls[1] >= 50:
        print(f"  âš ï¸  è·¨æ—¶é—´æ®µæ³›åŒ–èƒ½åŠ›ä¸€èˆ¬ (Recall@1 = {recalls[1]:.2f}%)")
        print(f"  â†’ å»ºè®®: è°ƒæ•´è¶…å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
    else:
        print(f"  âŒ è·¨æ—¶é—´æ®µæ³›åŒ–èƒ½åŠ›è¾ƒå·® (Recall@1 = {recalls[1]:.2f}%)")
        print(f"  â†’ å»ºè®®: æ£€æŸ¥æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒç­–ç•¥")

    # ä¿å­˜ç»“æœ
    results = {
        'evaluation_type': 'cross_temporal',
        'database_sessions': '190-194',
        'query_sessions': '195-199',
        'num_database': len(db_features),
        'num_queries': len(query_features),
        'recalls': recalls,
        'rank_stats': rank_stats,
        'checkpoint': checkpoint_path,
        'checkpoint_epoch': int(checkpoint['epoch']) + 1,
        'checkpoint_val_loss': float(checkpoint['metric'])
    }

    os.makedirs('logs/day9_evaluation_cross_temporal', exist_ok=True)
    result_file = 'logs/day9_evaluation_cross_temporal/evaluation_results-4.json'

    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {result_file}")

    # å¯¹æ¯”åˆ†æï¼ˆå¦‚æœæœ‰è®­ç»ƒé›†è‡ªæŸ¥è¯¢ç»“æœï¼‰
    train_self_result_file = 'logs/day9_evaluation_self/train_self_query-4.json'
    if os.path.exists(train_self_result_file):
        with open(train_self_result_file, 'r') as f:
            train_self_results = json.load(f)

        # ä¿®å¤ï¼šå¤„ç†JSONä¸­keyå¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ
        recalls_dict = train_self_results['recalls']
        if isinstance(list(recalls_dict.keys())[0], str):
            # keyæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºæ•´æ•°
            train_self_recall1 = float(recalls_dict['1'])
        else:
            # keyå·²ç»æ˜¯æ•´æ•°
            train_self_recall1 = recalls_dict[1]

        print("\n" + "=" * 60)
        print("å¯¹æ¯”åˆ†æ")
        print("=" * 60)
        print(f"è®­ç»ƒé›†è‡ªæŸ¥è¯¢ Recall@1: {train_self_recall1:.2f}%")
        print(f"è·¨æ—¶é—´æ®µè¯„ä¼° Recall@1: {recalls[1]:.2f}%")
        print(f"æ€§èƒ½å·®è·: {train_self_recall1 - recalls[1]:.2f}%")

        if abs(train_self_recall1 - recalls[1]) < 10:
            print("\nâœ… æ³›åŒ–èƒ½åŠ›ä¼˜ç§€ï¼Œè®­ç»ƒé›†å’ŒéªŒè¯é›†æ€§èƒ½æ¥è¿‘!")
        elif abs(train_self_recall1 - recalls[1]) < 20:
            print("\nâš ï¸  å­˜åœ¨ä¸€å®šçš„æ³›åŒ–gapï¼Œå¯æ¥å—")
        else:
            print("\nâŒ æ³›åŒ–èƒ½åŠ›å·®ï¼Œå­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆ")


if __name__ == "__main__":
    main()